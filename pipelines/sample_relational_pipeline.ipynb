{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/pandas/__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import pyspark.pandas as pd\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .appName(\"Relational Sample PySpark SQL Server Connection\") \\\n",
    "        .config(\"spark.jars\", \"../jdbc/mssql-jdbc-12.6.1.jre8.jar\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_read_from_db(table_name):\n",
    "    server_name = \"mssql\"\n",
    "    port = \"1433\"\n",
    "    database_name = \"Data\"\n",
    "    url = f\"jdbc:sqlserver://{server_name}:{port};databaseName={database_name}\"\n",
    "    username = \"SA\"\n",
    "    password = \"YourStrongPassword123\"\n",
    "    df = spark.read \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", url) \\\n",
    "        .option(\"dbtable\", table_name) \\\n",
    "        .option(\"user\", username) \\\n",
    "        .option(\"password\", password) \\\n",
    "        .option(\"encrypt\", \"false\") \\\n",
    "        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "        .load()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties = spark_read_from_db(\"ARProperties\")\n",
    "clients = spark_read_from_db(\"ARClients\")\n",
    "properties_clients = spark_read_from_db(\"PropertiesClients\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_condition_properties = properties[\"id\"] == properties_clients[\"id_prop\"]\n",
    "join_condition_clients = clients[\"id\"] == properties_clients[\"id_client\"]\n",
    "\n",
    "joined_df = properties_clients.join(clients, join_condition_clients) \\\n",
    "                              .join(properties, join_condition_properties)\n",
    "columns_to_drop = [\"id\"]\n",
    "joined_df = joined_df.drop(*columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkTypeError",
     "evalue": "[NOT_BOOL_OR_FLOAT_OR_INT] Argument `withReplacement (optional), fraction (required) and seed (optional)` should be a bool, float or str, got int, NoneType, NoneType.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mjoined_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:1948\u001b[0m, in \u001b[0;36mDataFrame.sample\u001b[0;34m(self, withReplacement, fraction, seed)\u001b[0m\n\u001b[1;32m   1942\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\n\u001b[1;32m   1943\u001b[0m     is_withReplacement_set\n\u001b[1;32m   1944\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m is_withReplacement_omitted_kwargs\n\u001b[1;32m   1945\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m is_withReplacement_omitted_args\n\u001b[1;32m   1946\u001b[0m ):\n\u001b[1;32m   1947\u001b[0m     argtypes \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mtype\u001b[39m(arg)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m [withReplacement, fraction, seed]]\n\u001b[0;32m-> 1948\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   1949\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL_OR_FLOAT_OR_INT\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1950\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m   1951\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwithReplacement (optional), \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1952\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfraction (required) and seed (optional)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1953\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(argtypes),\n\u001b[1;32m   1954\u001b[0m         },\n\u001b[1;32m   1955\u001b[0m     )\n\u001b[1;32m   1957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_withReplacement_omitted_args:\n\u001b[1;32m   1958\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m fraction \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mPySparkTypeError\u001b[0m: [NOT_BOOL_OR_FLOAT_OR_INT] Argument `withReplacement (optional), fraction (required) and seed (optional)` should be a bool, float or str, got int, NoneType, NoneType."
     ]
    }
   ],
   "source": [
    "joined_df.sample(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
